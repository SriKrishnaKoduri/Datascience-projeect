{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriKrishnaKoduri/Datascience-projeect/blob/main/Internship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 Package Installation & Imports  \n",
        "This section installs necessary packages (`prophet`, `mlxtend`) and imports all required libraries such as `pandas`, `numpy`, `sklearn`, `seaborn`, `plotly`, and more.\n"
      ],
      "metadata": {
        "id": "kFVPVwzr8O8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0-ojPqw2AsP"
      },
      "outputs": [],
      "source": [
        "# 1.1 Install if needed (Prophet)\n",
        "!pip install prophet mlxtend --quiet\n",
        "\n",
        "# 1.2 Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from prophet import Prophet\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from datetime import timedelta\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "My9YEMZX5Etu",
        "outputId": "52807710-c658-46c5-c5e1-dc3990043c5e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26a888de-5540-4553-a6ee-83c6dcf81828\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26a888de-5540-4553-a6ee-83c6dcf81828\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3767293195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2.1 Upload dataset from local\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2.2 Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 2.1 Upload dataset from local\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 2.2 Load dataset\n",
        "df = pd.read_csv(\"retail_data.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁 Data Upload and Initial Exploration  \n",
        "Here we upload the retail dataset from the local system and load it into a pandas DataFrame for further analysis.\n"
      ],
      "metadata": {
        "id": "1KENOo7v8Trx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igTd9ETE5JoC"
      },
      "outputs": [],
      "source": [
        "# 3.1 Convert date\n",
        "df['Date of Purchase'] = pd.to_datetime(df['Date of Purchase'])\n",
        "\n",
        "# 3.2 KNN Imputation\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# 3.2.1 (Optional) Iterative Imputer using MICE\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Uncomment this block if you want to use Iterative Imputer instead of KNN\n",
        "# mice_imputer = IterativeImputer(random_state=0)\n",
        "# df[numeric_cols] = mice_imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# 3.3 Outlier Handling (Tukey’s method)\n",
        "def cap_outliers(col):\n",
        "    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df[col] = np.clip(df[col], lower, upper)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    cap_outliers(col)\n",
        "\n",
        "# 3.3.1 (Optional) Robust Z-Score method for outlier detection\n",
        "from scipy.stats import median_abs_deviation\n",
        "\n",
        "def robust_z_score(series):\n",
        "    median = np.median(series)\n",
        "    mad = median_abs_deviation(series)\n",
        "    return (series - median) / (1.4826 * mad)\n",
        "\n",
        "# Flagging values with RZS > 3 as outliers\n",
        "for col in numeric_cols:\n",
        "    rzs = robust_z_score(df[col])\n",
        "    df[col] = np.where(np.abs(rzs) > 3, np.sign(rzs) * 3, df[col])  # capping at ±3\n",
        "\n",
        "# 3.4 Standardization\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# 3.4.1 (Optional) Min-Max Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Uncomment to try Min-Max instead of StandardScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# 3.5 Feature Engineering\n",
        "today = df['Date of Purchase'].max()\n",
        "recency = df.groupby('Customer ID')['Date of Purchase'].max().apply(lambda x: (today - x).days)\n",
        "frequency = df.groupby('Customer ID')['Invoice No'].nunique()\n",
        "monetary = df.groupby('Customer ID')['Selling Price'].sum()\n",
        "rfm = pd.DataFrame({'Recency': recency, 'Frequency': frequency, 'Monetary': monetary})\n",
        "rfm = rfm.merge(df[['Customer ID', 'Loyalty Score']].drop_duplicates(), on='Customer ID')\n",
        "\n",
        "# 3.5.1 Approximate Customer Lifetime Value (CLV)\n",
        "# Assumptions: Avg Order Value * Purchase Frequency * (Profit Margin)\n",
        "df['Total'] = df['Selling Price']  # Assuming 'Selling Price' = order value\n",
        "clv = df.groupby('Customer ID').agg({\n",
        "    'Invoice No': 'nunique',\n",
        "    'Selling Price': ['sum', 'mean']\n",
        "})\n",
        "clv.columns = ['Purchase_Frequency', 'Total_Revenue', 'Avg_Order_Value']\n",
        "\n",
        "# Assuming profit margin of 10%\n",
        "clv['CLV'] = clv['Avg_Order_Value'] * clv['Purchase_Frequency'] * 0.10\n",
        "\n",
        "# Merge back with RFM\n",
        "rfm = rfm.merge(clv['CLV'], left_index=True, right_index=True)\n",
        "\n",
        "# 3.5.2 Average Purchase Frequency\n",
        "# Frequency / Time span (in months)\n",
        "first_purchase = df.groupby('Customer ID')['Date of Purchase'].min()\n",
        "last_purchase = df.groupby('Customer ID')['Date of Purchase'].max()\n",
        "purchase_span_months = (last_purchase - first_purchase).dt.days / 30\n",
        "\n",
        "avg_frequency = frequency / purchase_span_months\n",
        "rfm['Avg_Purchase_Frequency'] = avg_frequency\n",
        "\n",
        "# 3.5.3 Discount Utilization Rate\n",
        "# Assuming you have 'Discount Amount' and 'Selling Price' columns\n",
        "\n",
        "if 'Discount Amount' in df.columns and 'Selling Price' in df.columns:\n",
        "    df['Discount Utilization'] = df['Discount Amount'] / (df['Selling Price'] + df['Discount Amount'])\n",
        "    discount_util = df.groupby('Customer ID')['Discount Utilization'].mean().fillna(0)\n",
        "    rfm = rfm.merge(discount_util.rename('Avg_Discount_Utilization'), on='Customer ID')\n",
        "else:\n",
        "    print(\"Discount Amount column not found. Skipping discount utilization calculation.\")\n",
        "\n",
        "# 3.5.4 Payment Method Preference\n",
        "# Assuming 'Payment Method' column is available\n",
        "\n",
        "if 'Payment Method' in df.columns:\n",
        "    payment_pref = df.groupby('Customer ID')['Payment Method'].agg(lambda x: x.value_counts().idxmax())\n",
        "    rfm = rfm.merge(payment_pref.rename('Preferred_Payment_Method'), on='Customer ID')\n",
        "else:\n",
        "    print(\"Payment Method column not found. Skipping payment preference calculation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loyalty Score based on frequency and monetary value\n",
        "rfm['Loyalty Score'] = pd.qcut(rfm['Frequency'], q=4, labels=[1, 2, 3, 4]).astype(int)\n",
        "\n",
        "# Customer Lifetime Value (CLV = Frequency × Monetary)\n",
        "rfm['CLV'] = rfm['Frequency'] * rfm['Monetary']\n",
        "\n",
        "# Discount Utilization Rate (if 'Discount' column exists)\n",
        "if 'Discount' in df.columns:\n",
        "    discount_utilization = df.groupby('Customer ID')['Discount'].mean()\n",
        "    rfm['Discount Utilization Rate'] = discount_utilization\n",
        "\n",
        "# Payment Method Preference (if 'Payment Method' column exists)\n",
        "if 'Payment Method' in df.columns:\n",
        "    preferred_payment = df.groupby('Customer ID')['Payment Method'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
        "    rfm['Payment Method Preference'] = preferred_payment\n"
      ],
      "metadata": {
        "id": "lEfFZjgd6kWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧹 Data Preprocessing  \n",
        "We perform preprocessing such as date conversion, handling missing values using `KNNImputer`, and standardizing numerical features.\n"
      ],
      "metadata": {
        "id": "n_JPXF728alI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91EWiGL15OG7"
      },
      "outputs": [],
      "source": [
        "# 4.1 Recency Histogram\n",
        "sns.histplot(rfm['Recency'], bins=30, kde=True)\n",
        "plt.title(\"Recency Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# 4.2 Monthly Sales\n",
        "monthly_sales = df.set_index('Date of Purchase').resample('M')['Selling Price'].sum()\n",
        "monthly_sales.plot(title=\"Monthly Sales Over Time\")\n",
        "plt.show()\n",
        "\n",
        "# 4.3 STL Decomposition\n",
        "stl = STL(monthly_sales, seasonal=13)\n",
        "res = stl.fit()\n",
        "res.plot()\n",
        "plt.show()\n",
        "\n",
        "# 4.4 Correlation Matrix\n",
        "sns.heatmap(df.corr(), annot=True, fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SARIMA Forecasting\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Train-test split\n",
        "train = monthly_sales.iloc[:-12]\n",
        "test = monthly_sales.iloc[-12:]\n",
        "\n",
        "# Fit SARIMA Model\n",
        "sarima_model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "# Forecast\n",
        "sarima_forecast = sarima_result.predict(start=test.index[0], end=test.index[-1])\n",
        "\n",
        "# Plot SARIMA forecast\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train.index, train, label='Train')\n",
        "plt.plot(test.index, test, label='Test')\n",
        "plt.plot(test.index, sarima_forecast, label='SARIMA Forecast')\n",
        "plt.legend()\n",
        "plt.title(\"SARIMA Forecast vs Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ONzfiSpy2l86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prophet Forecasting with Custom Seasonality\n",
        "df_prophet = monthly_sales.reset_index()\n",
        "df_prophet.columns = ['ds', 'y']\n",
        "\n",
        "# Define holiday example\n",
        "holidays = pd.DataFrame({\n",
        "    'holiday': 'festival',\n",
        "    'ds': pd.to_datetime(['2021-12-25', '2022-01-01', '2022-10-24']),\n",
        "    'lower_window': 0,\n",
        "    'upper_window': 1,\n",
        "})\n",
        "\n",
        "# Initialize Prophet with holidays\n",
        "m = Prophet(holidays=holidays)\n",
        "m.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
        "m.fit(df_prophet)\n",
        "\n",
        "# Forecast\n",
        "future = m.make_future_dataframe(periods=12, freq='M')\n",
        "forecast = m.predict(future)\n",
        "\n",
        "# Plot\n",
        "fig1 = m.plot(forecast)\n",
        "plt.title(\"Prophet Forecast with Custom Seasonality and Holidays\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p6Tk-C2M2qg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛠️ Feature Engineering  \n",
        "In this section, we derive new features including RFM metrics (Recency, Frequency, Monetary), Customer Lifetime Value (CLV), and Loyalty Score.\n"
      ],
      "metadata": {
        "id": "Zh5W2Bk68ejD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Monthly sales aggregation\n",
        "monthly_sales = df.resample('M', on='Date of Purchase')['Selling Price'].sum()\n",
        "\n",
        "# Train-test split\n",
        "train = monthly_sales[:-12]\n",
        "test = monthly_sales[-12:]\n",
        "\n",
        "# SARIMA Model (adjust order as needed)\n",
        "sarima_model = SARIMAX(train, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "# Forecasting\n",
        "sarima_forecast = sarima_result.forecast(steps=12)\n",
        "sarima_forecast.index = test.index\n",
        "\n",
        "# Evaluation\n",
        "sarima_rmse = mean_squared_error(test, sarima_forecast, squared=False)\n",
        "sarima_mae = mean_absolute_error(test, sarima_forecast)\n",
        "\n",
        "print(\"SARIMA RMSE:\", sarima_rmse)\n",
        "print(\"SARIMA MAE:\", sarima_mae)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(train.index, train, label='Train')\n",
        "plt.plot(test.index, test, label='Test')\n",
        "plt.plot(sarima_forecast.index, sarima_forecast, label='SARIMA Forecast')\n",
        "plt.legend()\n",
        "plt.title(\"SARIMA Forecast vs Actuals\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "76pf6Sod6pIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM for Time-Series Forecasting\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Prepare data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_series = scaler.fit_transform(monthly_sales.values.reshape(-1, 1))\n",
        "\n",
        "# Sequence generator\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i + seq_length])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X, y = create_sequences(scaled_series, seq_length)\n",
        "\n",
        "# Train-test split\n",
        "split = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=30, verbose=0)\n",
        "\n",
        "# Forecast\n",
        "lstm_pred = model.predict(X_test)\n",
        "lstm_pred_inv = scaler.inverse_transform(lstm_pred)\n",
        "y_test_inv = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot\n",
        "plt.plot(y_test_inv, label='Actual')\n",
        "plt.plot(lstm_pred_inv, label='LSTM Forecast')\n",
        "plt.title(\"LSTM Forecast vs Actual\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7kGuOgJ32uM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def print_metrics(true, pred, model_name):\n",
        "    mae = mean_absolute_error(true, pred)\n",
        "    rmse = np.sqrt(mean_squared_error(true, pred))\n",
        "    mape = np.mean(np.abs((true - pred) / true)) * 100\n",
        "    print(f\"{model_name}:\\n  MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\\n\")\n",
        "\n",
        "# Evaluate SARIMA\n",
        "print_metrics(test.values, sarima_forecast.values, \"SARIMA\")\n",
        "\n",
        "# Evaluate Prophet\n",
        "prophet_pred = forecast.set_index('ds')['yhat'][-12:]\n",
        "print_metrics(test.values, prophet_pred.values, \"Prophet\")\n",
        "\n",
        "# Evaluate LSTM\n",
        "print_metrics(y_test_inv.flatten(), lstm_pred_inv.flatten(), \"LSTM\")\n"
      ],
      "metadata": {
        "id": "DTdkkclq2xlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 Exploratory Data Analysis (EDA)  \n",
        "We use visualizations (Seaborn, Plotly) and statistical summaries to uncover trends in purchasing behavior, customer demographics, and seasonal effects.\n"
      ],
      "metadata": {
        "id": "HI4-yDDX8jr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Prepare monthly sales data\n",
        "sales = monthly_sales.values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_sales = scaler.fit_transform(sales)\n",
        "\n",
        "# Create sequences for LSTM\n",
        "def create_sequences(data, seq_len):\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - seq_len):\n",
        "        x.append(data[i:i+seq_len])\n",
        "        y.append(data[i+seq_len])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "seq_length = 12\n",
        "X, y = create_sequences(scaled_sales, seq_length)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, y_train = X[:-12], y[:-12]\n",
        "X_test, y_test = X[-12:], y[-12:]\n",
        "\n",
        "# Build and train LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=50, verbose=0)\n",
        "\n",
        "# Forecast\n",
        "lstm_preds = model.predict(X_test)\n",
        "lstm_preds_rescaled = scaler.inverse_transform(lstm_preds)\n",
        "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(y_test_rescaled, label='Actual')\n",
        "plt.plot(lstm_preds_rescaled, label='LSTM Forecast')\n",
        "plt.legend()\n",
        "plt.title(\"LSTM Sales Forecast\")\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "print(\"LSTM RMSE:\", mean_squared_error(y_test_rescaled, lstm_preds_rescaled, squared=False))\n",
        "print(\"LSTM MAE:\", mean_absolute_error(y_test_rescaled, lstm_preds_rescaled))\n"
      ],
      "metadata": {
        "id": "VRobF--W6uvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧩 Customer Segmentation  \n",
        "Using clustering algorithms like Gaussian Mixture Models and Agglomerative Clustering, we segment customers based on RFM and behavioral features.\n"
      ],
      "metadata": {
        "id": "5hWohJZ58qsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEZxrze15Q4C"
      },
      "outputs": [],
      "source": [
        "# 5.1 Gaussian Mixture Model Clustering\n",
        "X_seg = rfm[['Recency', 'Frequency', 'Monetary', 'Loyalty Score']]\n",
        "gmm = GaussianMixture(n_components=4, random_state=0)\n",
        "rfm['Segment'] = gmm.fit_predict(X_seg)\n",
        "\n",
        "# 5.2 Visualize Clusters\n",
        "sns.pairplot(rfm, hue='Segment', vars=['Recency', 'Frequency', 'Monetary'])\n",
        "plt.suptitle(\"Customer Segments\")\n",
        "plt.show()\n",
        "\n",
        "# 5.3 Customer Segment Profiling\n",
        "segment_profiles = rfm.groupby('Segment').agg({\n",
        "    'Recency': 'mean',\n",
        "    'Frequency': 'mean',\n",
        "    'Monetary': 'mean',\n",
        "    'Loyalty Score': 'mean',\n",
        "    'CLV': 'mean' if 'CLV' in rfm.columns else 'sum',\n",
        "    'Avg_Discount_Utilization': 'mean' if 'Avg_Discount_Utilization' in rfm.columns else 'sum'\n",
        "})\n",
        "\n",
        "print(\"Segment Profiles (mean values):\")\n",
        "display(segment_profiles)\n",
        "\n",
        "# Optional: View payment method preference counts\n",
        "if 'Preferred_Payment_Method' in rfm.columns:\n",
        "    payment_distribution = rfm.groupby('Segment')['Preferred_Payment_Method'].value_counts(normalize=True).unstack().fillna(0)\n",
        "    print(\"Payment Preferences by Segment:\")\n",
        "    display(payment_distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔮 Sales Forecasting  \n",
        "We forecast future sales using the Prophet time series model with custom seasonality and holiday effects.\n"
      ],
      "metadata": {
        "id": "lAe_pwA98wxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5B2vH3P5THB"
      },
      "outputs": [],
      "source": [
        "# 6.1 Prepare for Prophet\n",
        "sales_df = df[['Date of Purchase', 'Selling Price']].rename(columns={'Date of Purchase': 'ds', 'Selling Price': 'y'})\n",
        "daily_sales = sales_df.groupby('ds').sum().reset_index()\n",
        "\n",
        "# 6.2 Fit Model\n",
        "model = Prophet()\n",
        "model.fit(daily_sales)\n",
        "\n",
        "# 6.3 Forecast Future\n",
        "future = model.make_future_dataframe(periods=90)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# 6.4 Plot Forecast\n",
        "model.plot(forecast)\n",
        "plt.title(\"Sales Forecast (90 days ahead)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚠️ Customer Churn Prediction  \n",
        "Using machine learning models like Random Forest, we predict which customers are at risk of churning.\n"
      ],
      "metadata": {
        "id": "s6n3wtJr80iE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSCea7Mu5WYF"
      },
      "outputs": [],
      "source": [
        "# 7.1 Label Churn (no purchase in 90+ days)\n",
        "last_purchase = df.groupby('Customer ID')['Date of Purchase'].max()\n",
        "churn_threshold = df['Date of Purchase'].max() - timedelta(days=90)\n",
        "rfm['Churn'] = last_purchase < churn_threshold\n",
        "rfm['Churn'] = rfm['Churn'].astype(int)\n",
        "\n",
        "# 7.2 Model Training\n",
        "X = rfm[['Recency', 'Frequency', 'Monetary', 'Loyalty Score']]\n",
        "y = rfm['Churn']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7.3 Evaluation\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define X and y\n",
        "X = df.drop(columns=['Churn'])\n",
        "y = df['Churn']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "jok_v_t73O3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "log_reg_preds = log_reg.predict(X_test)\n"
      ],
      "metadata": {
        "id": "dIM41Pw_3TSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "dtree_preds = dtree.predict(X_test)\n"
      ],
      "metadata": {
        "id": "rtj78ZCS3WBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛒 Market Basket Analysis & Cross-Selling  \n",
        "In this final section, we apply Association Rule Mining using the Apriori algorithm to uncover product bundling opportunities and cross-selling strategies.\n"
      ],
      "metadata": {
        "id": "IxygVsAg86EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwI4OZHm5Y70"
      },
      "outputs": [],
      "source": [
        "# 8.1 Create Basket Matrix\n",
        "basket = df.groupby(['Invoice No', 'Product ID'])['Purchase Quantity'].sum().unstack().fillna(0)\n",
        "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# 8.2 Association Rules\n",
        "frequent_items = apriori(basket, min_support=0.01, use_colnames=True)\n",
        "rules = association_rules(frequent_items, metric=\"lift\", min_threshold=1)\n",
        "rules.sort_values(by='confidence', ascending=False).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📈 Seasonal Decomposition  \n",
        "Here we apply STL decomposition to identify trends and seasonality in the time series sales data.\n"
      ],
      "metadata": {
        "id": "1yZO4bFb8m93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Strategic Insights:\n",
        "\n",
        "- **SARIMA and Prophet** show seasonal trends with a peak during festival months. Stock inventory accordingly.\n",
        "- **LSTM** captures trend but might lag if data is noisy or highly seasonal.\n",
        "- **Dip detected** in off-season months (e.g., April-June) — consider running promotional campaigns.\n",
        "- **Recommendation:** Increase stock and marketing efforts during November-December and major holidays.\n"
      ],
      "metadata": {
        "id": "DCrtd61d22Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "Yd-FJw-y3nol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred):\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
        "    print(\"AUC-ROC:\", roc_auc_score(y_true, y_pred))\n",
        "\n",
        "# Evaluate all models\n",
        "evaluate_model(\"Logistic Regression\", y_test, log_reg_preds)\n",
        "evaluate_model(\"Decision Tree\", y_test, dtree_preds)\n",
        "evaluate_model(\"Random Forest\", y_test, rf_preds)\n",
        "evaluate_model(\"XGBoost\", y_test, xgb_preds)\n"
      ],
      "metadata": {
        "id": "2Y2E54st3q1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(model, X_test, y_test, label):\n",
        "    y_score = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # ROC Curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc_score(y_test, y_score):.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Precision-Recall Curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(recall, precision, label=label)\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each\n",
        "plot_curves(log_reg, X_test, y_test, \"Logistic Regression\")\n",
        "plot_curves(dtree, X_test, y_test, \"Decision Tree\")\n",
        "plot_curves(rf_best, X_test, y_test, \"Random Forest\")\n",
        "plot_curves(xgb_model, X_test, y_test, \"XGBoost\")\n"
      ],
      "metadata": {
        "id": "s2YKn-JU3tQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5mzva3+aXmIT5RwewN8ky",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}